{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "020d10c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/HANNA1/Documents/Documents/anaconda3/lib/python3.11/site-packages (4.12.0.88)\r\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /Users/HANNA1/Documents/Documents/anaconda3/lib/python3.11/site-packages (from opencv-python) (2.2.6)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9036cf9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3\n",
      "  Obtaining dependency information for numpy==1.24.3 from https://files.pythonhosted.org/packages/ee/6c/7217a8844dfe22e349bccbecd35571fa72c5d7fe8b33d8c5540e8cc2535c/numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (5.6 kB)\n",
      "Collecting pandas==2.0.3\n",
      "  Obtaining dependency information for pandas==2.0.3 from https://files.pythonhosted.org/packages/8f/bb/aea1fbeed5b474cb8634364718abe9030d7cc7a30bf51f40bd494bbc89a2/pandas-2.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/HANNA1/Documents/Documents/anaconda3/lib/python3.11/site-packages (from pandas==2.0.3) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/HANNA1/Documents/Documents/anaconda3/lib/python3.11/site-packages (from pandas==2.0.3) (2024.1)\n",
      "Collecting tzdata>=2022.1 (from pandas==2.0.3)\n",
      "  Obtaining dependency information for tzdata>=2022.1 from https://files.pythonhosted.org/packages/5c/23/c7abc0ca0a1526a0774eca151daeb8de62ec457e77262b66b359c3c7679e/tzdata-2025.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/HANNA1/Documents/Documents/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas==2.0.3) (1.16.0)\n",
      "Downloading numpy-1.24.3-cp311-cp311-macosx_11_0_arm64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m583.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp311-cp311-macosx_11_0_arm64.whl (10.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m541.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m829.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tzdata, numpy, pandas\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tables 3.8.0 requires blosc2~=2.0.0, which is not installed.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.24.3 which is incompatible.\n",
      "nemo-toolkit 1.21.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n",
      "transformers 4.29.2 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.20.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.24.3 pandas-2.0.3 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy==1.24.3 pandas==2.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8410f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62beba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce9e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SIZE = 48\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 40\n",
    "NUM_CLASSES = 7\n",
    "CLASSES = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "DATA_DIR = 'fer2013'\n",
    "MODEL_PATH = 'facial_expression_model.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2079b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for dataset\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    logger.error(f\"Dataset directory '{DATA_DIR}' not found. Please download FER2013 from Kaggle.\")\n",
    "    raise FileNotFoundError(f\"Directory '{DATA_DIR}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8626674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation and preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37d49932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Load training and validation data\n",
    "try:\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'train'),\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=CLASSES\n",
    "    )\n",
    "    validation_generator = test_datagen.flow_from_directory(\n",
    "        os.path.join(DATA_DIR, 'test'),\n",
    "        target_size=(IMG_SIZE, IMG_SIZE),\n",
    "        color_mode='grayscale',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=CLASSES\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading dataset: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e49e42c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build CNN model\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe34808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 21:41:36,356 - INFO - Building and training model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 59ms/step - accuracy: 0.2558 - loss: 2.0796 - val_accuracy: 0.3323 - val_loss: 1.6702\n",
      "Epoch 2/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 60ms/step - accuracy: 0.3461 - loss: 1.6460 - val_accuracy: 0.4039 - val_loss: 1.5016\n",
      "Epoch 3/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 60ms/step - accuracy: 0.3991 - loss: 1.5381 - val_accuracy: 0.4374 - val_loss: 1.4085\n",
      "Epoch 4/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 60ms/step - accuracy: 0.4202 - loss: 1.4787 - val_accuracy: 0.4482 - val_loss: 1.4298\n",
      "Epoch 5/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 64ms/step - accuracy: 0.4433 - loss: 1.4394 - val_accuracy: 0.4646 - val_loss: 1.3711\n",
      "Epoch 6/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 60ms/step - accuracy: 0.4625 - loss: 1.4032 - val_accuracy: 0.4802 - val_loss: 1.3602\n",
      "Epoch 7/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 60ms/step - accuracy: 0.4740 - loss: 1.3809 - val_accuracy: 0.5274 - val_loss: 1.2337\n",
      "Epoch 8/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.4801 - loss: 1.3529 - val_accuracy: 0.5270 - val_loss: 1.2483\n",
      "Epoch 9/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 62ms/step - accuracy: 0.4906 - loss: 1.3323 - val_accuracy: 0.5117 - val_loss: 1.2577\n",
      "Epoch 10/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 62ms/step - accuracy: 0.4988 - loss: 1.3177 - val_accuracy: 0.5149 - val_loss: 1.2847\n",
      "Epoch 11/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 63ms/step - accuracy: 0.5061 - loss: 1.2977 - val_accuracy: 0.5330 - val_loss: 1.2053\n",
      "Epoch 12/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.5111 - loss: 1.2897 - val_accuracy: 0.5261 - val_loss: 1.2810\n",
      "Epoch 13/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.5159 - loss: 1.2666 - val_accuracy: 0.5506 - val_loss: 1.1715\n",
      "Epoch 14/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 62ms/step - accuracy: 0.5308 - loss: 1.2612 - val_accuracy: 0.5164 - val_loss: 1.2308\n",
      "Epoch 15/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.5336 - loss: 1.2479 - val_accuracy: 0.5542 - val_loss: 1.1828\n",
      "Epoch 16/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5260 - loss: 1.2527 - val_accuracy: 0.5163 - val_loss: 1.2478\n",
      "Epoch 17/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5443 - loss: 1.2140 - val_accuracy: 0.5645 - val_loss: 1.1527\n",
      "Epoch 18/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 62ms/step - accuracy: 0.5414 - loss: 1.2095 - val_accuracy: 0.5311 - val_loss: 1.2249\n",
      "Epoch 19/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 64ms/step - accuracy: 0.5520 - loss: 1.2013 - val_accuracy: 0.5632 - val_loss: 1.1521\n",
      "Epoch 20/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 62ms/step - accuracy: 0.5491 - loss: 1.1917 - val_accuracy: 0.5698 - val_loss: 1.1594\n",
      "Epoch 21/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 60ms/step - accuracy: 0.5557 - loss: 1.1760 - val_accuracy: 0.5488 - val_loss: 1.1936\n",
      "Epoch 22/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.5584 - loss: 1.1752 - val_accuracy: 0.5748 - val_loss: 1.1441\n",
      "Epoch 23/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.5546 - loss: 1.1815 - val_accuracy: 0.5417 - val_loss: 1.2078\n",
      "Epoch 24/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 61ms/step - accuracy: 0.5589 - loss: 1.1589 - val_accuracy: 0.5635 - val_loss: 1.1491\n",
      "Epoch 25/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 62ms/step - accuracy: 0.5697 - loss: 1.1530 - val_accuracy: 0.5839 - val_loss: 1.1030\n",
      "Epoch 26/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 62ms/step - accuracy: 0.5636 - loss: 1.1576 - val_accuracy: 0.5687 - val_loss: 1.1462\n",
      "Epoch 27/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 63ms/step - accuracy: 0.5747 - loss: 1.1364 - val_accuracy: 0.5887 - val_loss: 1.0883\n",
      "Epoch 28/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 64ms/step - accuracy: 0.5807 - loss: 1.1222 - val_accuracy: 0.5680 - val_loss: 1.1505\n",
      "Epoch 29/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 64ms/step - accuracy: 0.5702 - loss: 1.1436 - val_accuracy: 0.5825 - val_loss: 1.1144\n",
      "Epoch 30/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5687 - loss: 1.1343 - val_accuracy: 0.5961 - val_loss: 1.0639\n",
      "Epoch 31/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5753 - loss: 1.1143 - val_accuracy: 0.6028 - val_loss: 1.0643\n",
      "Epoch 32/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 65ms/step - accuracy: 0.5785 - loss: 1.1198 - val_accuracy: 0.5939 - val_loss: 1.0754\n",
      "Epoch 33/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5833 - loss: 1.1003 - val_accuracy: 0.5847 - val_loss: 1.1123\n",
      "Epoch 34/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5926 - loss: 1.0921 - val_accuracy: 0.5740 - val_loss: 1.1414\n",
      "Epoch 35/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 63ms/step - accuracy: 0.5901 - loss: 1.0979 - val_accuracy: 0.5673 - val_loss: 1.1662\n",
      "Epoch 36/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5941 - loss: 1.0841 - val_accuracy: 0.5765 - val_loss: 1.1255\n",
      "Epoch 37/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 64ms/step - accuracy: 0.5964 - loss: 1.0787 - val_accuracy: 0.5802 - val_loss: 1.1146\n",
      "Epoch 38/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 68ms/step - accuracy: 0.5947 - loss: 1.0769 - val_accuracy: 0.6064 - val_loss: 1.0552\n",
      "Epoch 39/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 66ms/step - accuracy: 0.5963 - loss: 1.0729 - val_accuracy: 0.5794 - val_loss: 1.1387\n",
      "Epoch 40/40\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 67ms/step - accuracy: 0.5972 - loss: 1.0727 - val_accuracy: 0.5900 - val_loss: 1.0904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-18 22:19:18,322 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-07-18 22:19:18,393 - INFO - Model saved to facial_expression_model.h5\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "try:\n",
    "    logger.info(\"Building and training model...\")\n",
    "    model = build_model()\n",
    "    model.fit(\n",
    "        train_generator,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=validation_generator,\n",
    "        verbose=1\n",
    "    )\n",
    "    model.save(MODEL_PATH)\n",
    "    logger.info(f\"Model saved to {MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Training failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict expression from an image\n",
    "def predict_expression(image_path, model, cascade_path='haarcascade_frontalface_default.xml'):\n",
    "    try:\n",
    "        # Load Haar cascade for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + cascade_path)\n",
    "        if face_cascade.empty():\n",
    "            logger.error(\"Failed to load Haar cascade file.\")\n",
    "            raise FileNotFoundError(\"Haar cascade file not found.\")\n",
    "\n",
    "        # Load and preprocess image\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if img is None:\n",
    "            logger.error(f\"Failed to load image: {image_path}\")\n",
    "            raise ValueError(\"Invalid image path.\")\n",
    "        \n",
    "        faces = face_cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=5)\n",
    "        if len(faces) == 0:\n",
    "            logger.warning(\"No faces detected in the image.\")\n",
    "            return \"No face detected\"\n",
    "\n",
    "        # Process first detected face\n",
    "        (x, y, w, h) = faces[0]\n",
    "        face = img[y:y+h, x:x+w]\n",
    "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "        face = face.astype('float32') / 255.0\n",
    "        face = np.expand_dims(face, axis=(0, -1))  # Shape: (1, 48, 48, 1)\n",
    "\n",
    "        # Predict\n",
    "        prediction = model.predict(face)\n",
    "        expression = CLASSES[np.argmax(prediction)]\n",
    "        logger.info(f\"Predicted expression: {expression}\")\n",
    "        return expression\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error predicting expression: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4645efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for real-time webcam prediction\n",
    "def webcam_predict(model, cascade_path='haarcascade_frontalface_default.xml'):\n",
    "    try:\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + cascade_path)\n",
    "        if face_cascade.empty():\n",
    "            logger.error(\"Failed to load Haar cascade file.\")\n",
    "            raise FileNotFoundError(\"Haar cascade file not found.\")\n",
    "\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        if not cap.isOpened():\n",
    "            logger.error(\"Failed to open webcam.\")\n",
    "            raise RuntimeError(\"Webcam not accessible.\")\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                logger.warning(\"Failed to capture frame.\")\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                face = gray[y:y+h, x:x+w]\n",
    "                face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "                face = face.astype('float32') / 255.0\n",
    "                face = np.expand_dims(face, axis=(0, -1))\n",
    "                \n",
    "                prediction = model.predict(face)\n",
    "                expression = CLASSES[np.argmax(prediction)]\n",
    "                \n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, expression, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow('Facial Expression Recognition', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in webcam prediction: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb1aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load trained model for testing\n",
    "    try:\n",
    "        model = tf.keras.models.load_model(MODEL_PATH)\n",
    "        logger.info(\"Model loaded successfully!\")\n",
    "        \n",
    "        # Example: Predict expression from a single image\n",
    "        # Replace 'path_to_image.jpg' with an actual image path\n",
    "        # test_image = 'fer2013/test/happy/0001.jpg'\n",
    "        # result = predict_expression(test_image, model)\n",
    "        # print(f\"Predicted expression: {result}\")\n",
    "\n",
    "        # Real-time webcam prediction\n",
    "        logger.info(\"Starting webcam prediction (press 'q' to quit)...\")\n",
    "        webcam_predict(model)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da167da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
